
# Author MoosaHussain 
#Date of Submission 10/4/2020
​
​
import numpy as numpy
import nltk
from nltk.stem import WordNetLemmatizer 
from os import listdir
from os.path import isfile, join
import re
from nltk.stem import PorterStemmer 
global N 
N = 56
unique_words_all = []
​
def Term_Frequency(words):
    words_unique = []
    word_freq = {}
    for word in words:
        if word not in words_unique:
            words_unique.append(word)
    sor_uni = sorted(words_unique)
    for word in sor_uni:
        word_freq[word] = words.count(word)
    return word_freq
​
def lemma(words):
    
    lemmatizer = WordNetLemmatizer()
    wo =[]
    for word in words:
        wo.append(lemmatizer.lemmatize(word))
    return(wo) 
    
​
    
def getunique(list):
    unlist = []
    for x in list:
        if x not in unlist:
            unlist.append(x)
    return(unlist)        
​
​
                                # Initializing the Model 
​
word_freq_in_doc = {}
dict = {}
idx = 0
doc=[]
files=[]
NewList=[]
Total_Vocab =[]
                    # Loading stop Words and Files Needed for the Processing 
files =['speech_0.txt', 'speech_1.txt','speech_2.txt','speech_3.txt','speech_4.txt','speech_5.txt','speech_6.txt','speech_7.txt','speech_8.txt','speech_9.txt','speech_10.txt','speech_11.txt','speech_12.txt','speech_13.txt','speech_14.txt','speech_15.txt','speech_16.txt','speech_17.txt','speech_18.txt','speech_19.txt','speech_20.txt','speech_21.txt','speech_22.txt','speech_23.txt','speech_24.txt','speech_25.txt','speech_26.txt','speech_27.txt','speech_28.txt','speech_29.txt','speech_30.txt','speech_31.txt','speech_32.txt','speech_33.txt','speech_34.txt','speech_35.txt','speech_36.txt','speech_37.txt','speech_38.txt','speech_39.txt','speech_40.txt','speech_41.txt','speech_42.txt','speech_43.txt','speech_44.txt','speech_45.txt','speech_46.txt','speech_47.txt','speech_48.txt','speech_49.txt','speech_50.txt','speech_51.txt','speech_52.txt','speech_53.txt','speech_54.txt','speech_55.txt']
with open('Stopword-List.txt') as u : 
    Stopwords = u.read().split()
                    #Looping through Each File in the Directory 
                    
                #Pre Processing the Files 
for file in files:
    file = open(file, "r")
    words = file.read().split()
    stri = ' '.join(words) 
    words = re.sub("[^A-Za-z0-9]+"," ",stri)
    words = words.split()
    words = [word for word in words if len(words)>1]
                # Case Folding 
    words = [word.lower() for word in words]
            # Stop Words Removal        
    words = [word for word in words if word not in Stopwords]
                # Lemmatization     
    words = lemma(words)
    doc.append(words)
    r = Term_Frequency(words)
    
            #Formation of Index 
    word_freq_in_doc = getunique(words)
         
    for word in  word_freq_in_doc:
​
        NewList.append([word,idx,r[word]])
        
        
        Total_Vocab.append(word)
    idx = idx + 1 
total_unique_words = getunique(Total_Vocab)
print(N)
​
Finalise_List = NewList
​
​
​
#                                Save the Index in File 
with open('Index_Doc.txt',"w") as index:
    for f in Finalise_List:
        index.write('%s\n'% f )
    index.close()
#                                   Load the Index From File 
Final_List=[]
with open('Index_Doc.txt',"r") as Final_Index:
    Final_List=Final_Index.readlines()
import math
#                       Helper Functions For Further Processsing 
#                        Doc_Frequency

def Doc_frequency(q):
    
    Doc_Frequency = {}
    check = 0 
    for f in Finalise_List :
#        print(f[0])
        if f[0] ==  q :
            check =check + 1
 #           print(check)
    return(check) 


#Inverse_Doc_Frequency
def Inverse_Doc_Frequency(doc_fre):
    N = 56
    Inverse_Doc_Frequency= math.log10((int(doc_fre))/N)
    return(Inverse_Doc_Frequency)

def doc_list(word):
    c = 0 
    l=[]
    
    for i in doc :
        for j in i:
            if j == word :
                l.append(1)
                o = 1
                break
                
        if 0 == 0 :
            l.append(0)
        o=0
        c = c+1
    return(l)    
        
def Term_Freq_Doc(term,doc_id):
    i=0
    list_f=[]
    for f in doc:
        if i== doc_id :
            list_f = f 
        i=i+1   
    term_freq = Term_Frequency(list_f)       
   # print(term_freq)
   # y=term_freq[term]    
    l=0
    for i in term_freq :
        if i == term :
            l=term_freq[i]
            break
    return(l)

    
def TF_IDF(tf,idf):
    return(tf*idf)


def Doc_freq(Finalise_List):
    
    Doc_Frequency = {}
    check = 0 
    for f in Finalise_List :
        for i in Finalise_List :
            if f[0] ==  i[0] :
                check =check + 1
                
        Doc_Frequency[f[0]] = check 
        check = 0
         
    
    return(Doc_Frequency) 
#IDF For Documents 

def Inverse_Doc_Freq(doc_fre):
    Inverse_Doc_Frequency = {}
    check = 0 
#print(doc_fre)
    N=56
    for f in Finalise_List:
        t = f[0]
        Inverse_Doc_Frequency[f[0]]= math.log10((int(doc_fre))/N)
    return(Inverse_Doc_Frequency)
        

#TF * IDF MATRIX 
def TF_IDF_Matrix(Finalise_List):
    TF_IDF={}
    Final_Array= []
    DF=Doc_freq(Finalise_List)
    IDF=Inverse_Doc_Freq(DF)
    #print(IDF)
    for f in Finalise_List:
        TF_IDF[f[0]] = f[2] * IDF[f[0]]
        Final_Array.append([f[0],f[1],TF_IDF[f[0]]])
        
    return(Final_Array)
print("404")    
import math
#                       Helper Functions For Further Processsing 
#                        Doc_Frequency
​
def Doc_frequency(q):
    
    Doc_Frequency = {}
    check = 0 
    for f in Finalise_List :
#        print(f[0])
        if f[0] ==  q :
            check =check + 1
 #           print(check)
    return(check) 
​
​
#Inverse_Doc_Frequency
def Inverse_Doc_Frequency(doc_fre):
    N = 56
    Inverse_Doc_Frequency= math.log10((int(doc_fre))/N)
    return(Inverse_Doc_Frequency)
​
def doc_list(word):
    c = 0 
    l=[]
    
    for i in doc :
        for j in i:
            if j == word :
                l.append(1)
                o = 1
                break
                
        if 0 == 0 :
            l.append(0)
        o=0
        c = c+1
    return(l)    
        
def Term_Freq_Doc(term,doc_id):
    i=0
    list_f=[]
    for f in doc:
        if i== doc_id :
            list_f = f 
        i=i+1   
    term_freq = Term_Frequency(list_f)       
   # print(term_freq)
   # y=term_freq[term]    
    l=0
    for i in term_freq :
        if i == term :
            l=term_freq[i]
            break
    return(l)
​
    
def TF_IDF(tf,idf):
    return(tf*idf)
​
​
def Doc_freq(Finalise_List):
    
    Doc_Frequency = {}
    check = 0 
    for f in Finalise_List :
        for i in Finalise_List :
            if f[0] ==  i[0] :
                check =check + 1
                
        Doc_Frequency[f[0]] = check 
        check = 0
         
    
    return(Doc_Frequency) 
#IDF For Documents 
​
def Inverse_Doc_Freq(doc_fre):
    Inverse_Doc_Frequency = {}
    check = 0 
#print(doc_fre)
    N=56
    for f in Finalise_List:
        t = f[0]
        Inverse_Doc_Frequency[f[0]]= math.log10((int(doc_fre))/N)
    return(Inverse_Doc_Frequency)
        
​
#TF * IDF MATRIX 
def TF_IDF_Matrix(Finalise_List):
    TF_IDF={}
    Final_Array= []
    DF=Doc_freq(Finalise_List)
    IDF=Inverse_Doc_Freq(DF)
    #print(IDF)
    for f in Finalise_List:
        TF_IDF[f[0]] = f[2] * IDF[f[0]]
        Final_Array.append([f[0],f[1],TF_IDF[f[0]]])
        
    return(Final_Array)
print("404")    
404
​
# Calculating the TF_IDF Score of Documents Here! 
# Helper Function 
def helper(i):
    arr=[]
    for word in total_unique_words:
        x= Term_Freq_Doc(word,i)
        z=Doc_frequency(word)
        y=Inverse_Doc_Frequency(z)
        v = x * y
        arr.append(v)
    return(arr)
​
​
#Calculating And Storing In File 
#This Take Time So File of TF_IDF Attached With Assignment  
x=56
​
#with open('TF_IDF_SCORE_FDoc.txt',"w") as TF_IDF_SCORE:
 #   for i in range(x):
  #      print(i)
   #     TF_IDF_SCORE.write('%s\n' % helper(i))
    #    print(i)
    
​
​
# Comment this code because Processed File is Attached Use this 
    
​
#                                      Loading the TF_IDF SCORE of documents from File 
with open('TF_IDF_SCORE_FDoc.txt',"r") as doc_score:
    doc_S=[]
    doc_S = doc_score.readlines()
                        # Calculate the Magnitude of Vectors 
def Magnitude(doc_vec,query_vec):
    check = 0
    d_v=0
    q_v=0
    for i in range(len(doc_vec)):
            x=doc_vec[check]
            y=doc_vec[check]
            d_v=d_v+math.pow(float(x),2)
            q_v=q_v+math.pow(float(y),2)
            check = check +1 
            
    return(math.sqrt(d_v) * math.sqrt(q_v) )
                        # Filter  the Documents with Respect to Alpha Value 
def Refine(list, n):
    newarray=[]
    for i in list:
        if i[1] > n:
            newarray.append(i)
    return(newarray)       
                        # Sort The Documents With Respect to Cosin Similarity 
def Sorted_Documents(nlist):
    for passnum in range(len(nlist)-1,0,-1):
        for i in range(passnum):
            x=nlist[i][1]
            y=nlist[i+1][1]
            if x < y :
                temp = nlist[i]
                nlist[i] = nlist[i+1]
                nlist[i+1]= temp      
    l=[]
    for i in nlist :
        l.append(i[0])
    return(l)
                                # Query Vector 
​
def QueryVector(query):
    lemmatizer = WordNetLemmatizer()
    with open('Stopword-List.txt') as u : 
        Stopwords = u.read().split()
    query = query.split()
    query = [word for word in query if word not in Stopwords]
    query = [word.lower() for word in query]
    query = [lemmatizer.lemmatize(word) for word in query]
​
    TF_ARRAY = []
    res=[]
    queryvector=[]
    lenq=len(query)
    for i in total_unique_words:
        if lenq == 1 :
            if i == query[0]:
                tf=Term_Frequency(query)
                queryvector.append( tf[query[0]] * Inverse_Doc_Frequency(Doc_frequency(query[0])))
            else:
                queryvector.append(0)
    
            
        elif lenq == 2 :
            if i == query[0] or  i == query[1] :
                tf=Term_Frequency(query)
                if i== query[0] :
                    queryvector.append( tf[query[0]] * Inverse_Doc_Frequency(Doc_frequency(query[0])))
                elif i== query[1]:
                    queryvector.append( tf[query[1]] * Inverse_Doc_Frequency(Doc_frequency(query[1])))
                    
            else:
                queryvector.append(0)
        
        
        elif lenq == 3 :
            if i == query[0] or  i == query[1] or  i == query[2]:
                tf=Term_Frequency(query)
                if i== query[0] :
                    queryvector.append( tf[query[0]] * Inverse_Doc_Frequency(Doc_frequency(query[0])))
                elif i== query[1]:
                    queryvector.append( tf[query[1]] * Inverse_Doc_Frequency(Doc_frequency(query[1])))
                elif i== query[2]:
                    queryvector.append( tf[query[2]] * Inverse_Doc_Frequency(Doc_frequency(query[2])))
    
            else:
                queryvector.append(0)    
            
        elif lenq == 4 :
            if i == query[0] or  i == query[1] or  i == query[2]or  i == query[3]:
                tf=Term_Frequency(query)
                if i== query[0] :
                    queryvector.append( tf[query[0]] * Inverse_Doc_Frequency(Doc_frequency(query[0])))
                elif i== query[1]:
                    queryvector.append( tf[query[1]] * Inverse_Doc_Frequency(Doc_frequency(query[1])))
                elif i== query[2]:
                    queryvector.append( tf[query[2]] * Inverse_Doc_Frequency(Doc_frequency(query[2])))
                elif i== query[3]:
                    queryvector.append( tf[query[3]] * Inverse_Doc_Frequency(Doc_frequency(query[3])))
            else:
                queryvector.append(0)
            
            
        elif lenq == 5 :
            if i == query[0] or  i == query[1]or  i == query[2] or  i == query[3] or  i == query[4] :
                tf=Term_Frequency(query)
                if i== query[0] :
                    queryvector.append( tf[query[0]] * Inverse_Doc_Frequency(Doc_frequency(query[0])))
                elif i== query[1]:
                    queryvector.append( tf[query[1]] * Inverse_Doc_Frequency(Doc_frequency(query[1])))
                elif i== query[2]:
                    queryvector.append( tf[query[2]] * Inverse_Doc_Frequency(Doc_frequency(query[2])))
                elif i== query[3]:
                    queryvector.append( tf[query[3]] * Inverse_Doc_Frequency(Doc_frequency(query[3])))
                elif i== query[4]:
                    queryvector.append( tf[query[4]] * Inverse_Doc_Frequency(Doc_frequency(query[4])))
            else:
                
                queryvector.append(0)
            
            
        elif lenq == 6 :    
            if i == query[0] or  i == query[1] or  i == query[2] or  i == query[3] or  i == query[4]or  i == query[5]:
                tf=Term_Frequency(query)
                if i== query[0] :
                    queryvector.append( tf[query[0]] * Inverse_Doc_Frequency(Doc_frequency(query[0])))
                elif i== query[1]:
                    queryvector.append( tf[query[1]] * Inverse_Doc_Frequency(Doc_frequency(query[1])))
                elif i== query[2]:
                    queryvector.append( tf[query[2]] * Inverse_Doc_Frequency(Doc_frequency(query[2])))
                elif i== query[3]:
                    queryvector.append( tf[query[3]] * Inverse_Doc_Frequency(Doc_frequency(query[3])))
                elif i== query[4]:
                    queryvector.append( tf[query[4]] * Inverse_Doc_Frequency(Doc_frequency(query[4])))
                elif i== query[5]:
                    queryvector.append( tf[query[5]] * Inverse_Doc_Frequency(Doc_frequency(query[5])))
            else:
                queryvector.append(0)
        
        else:
            print("Not In Range")
    return(queryvector)
​
​
​
​
import math 
def QueryResult(query):
    
    queryvec= QueryVector(query) 
#Result Processing 
    
#queryvec is Query Vector 
#doc_S is Documents Vector 
    h = 0
    p=0
    g=0
    d=0
    Cosin_Similarity_Score = []
    for i in doc_S :
        d=d+1
        x = re.sub("[^,-.-0-9]+","",i)
        y=x.split(',')
        b=0
        p=0
        q=0
        #print(y)
        for j in y:   
            #print(queryvec[j])
            z=float(j)
            t= z * queryvec[b]
            #print(queryvec[b])
            b= b+1
           # print(t)
           # break
            p = p+float(t)
        
       # print("p = ",p) 
        q= p / Magnitude(y,queryvec)
      #  print("q = ",q)
        Cosin_Similarity_Score.append([g,q])
     #   print(Cosin_Similarity_Score)
        g=g+1
    
    #print(Cosin_Similarity_Score)
    
    list= Refine(Cosin_Similarity_Score,0.00005)   # Change The Value Of Alpha From Here ! 
    #print(list)
    V=Sorted_Documents(list)
    return(V)
​
​
​
​
​
​
import math
​
from tkinter import *
import tkinter.scrolledtext as tkst
#Graphical User Interphase for Assignment 
class GraphicalUserInterphase:
​
    def __init__(self, window):
        self.window = window
        self.window.configure(background = "slate blue")
        self.Outline()
        self.Buttons()
        
    def Outline(self):
​
        self.titleLabel = Label(self.window, text=" Information Retrieval Assignment 2 ", bg="lavender", font=("Helvetica", 24))
        self.titleLabel.grid(row=1, column=2,pady=10)
        self.titleLabel.grid(row=2, column=2,sticky='S',pady=10)
        self.queryLabel = Label(self.window, text="Search Query", bg="lavender", font=("Helvetica", 18))
        self.resultsLabel = Label(self.window, text="Results", bg="lavender", font=("Helvetica", 23))
        self.queryEntry = Entry(self.window, width=40)
        self.queryLabel.grid(row=3, column=1,pady=10,padx=5)
        self.queryEntry.grid(row=3, column=2, sticky='W',ipady=5)
        self.resultsLabel.grid(row=5, column=1,pady=20,padx=5)
        self.textArea = tkst.ScrolledText(master=self.window, wrap = WORD, width = 60, height=15)
        self.textArea.grid(row=7, column=0, columnspan=5)
​
    def Buttons(self):
​
        self.SearchBtn = Button(self.window, text="Search ", height=2, width=7 , command=self.Search)
        self.SearchBtn.grid(row=4, column=2, sticky='W', padx=10, pady=10)
        self.clearBtn = Button(self.window, text="Clear", height=2, width=7, command=self.Clear)
        self.clearBtn.grid(row=4, column=2, padx=10, pady=10)
        
    def Search(self):
        query = self.queryEntry.get()
        result =  QueryResult(query)
        
        if (len(result) == 0):
            self.textArea.insert(INSERT, "Empty Result set  " + query + "\n")
        else:
            self.textArea.insert(INSERT, "Query: " + query + "\n" + str(result) + "\n")
    def Clear(self):
        self.textArea.delete('1.0', END)
        self.queryEntry.delete("0", END)
​
window = Tk()
​
window.title("k173934 Information Retrieval Assignment 2 ")
obj = GraphicalUserInterphase(window)
window.mainloop()
​
​
